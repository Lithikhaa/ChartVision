# ChartVision


## DATASETS
The full ChartQA dataset (including the annotations) can be downloaded from the following huggingface dataset[ Full ChartQA Dataset](https://huggingface.co/datasets/ahmed-masry/ChartQA)

![image](https://github.com/user-attachments/assets/92ee346c-85a9-4e71-990f-072f4c05e0e1)


Each annotation JSON file follows a format similar to the PlotQA and FigureQA datasets. For more details, you can refer to the documentation:[PlotQA Dataset](https://github.com/NiteshMethani/PlotQA/blob/master/PlotQA_Dataset.md) and [FigureQA Dataset](https://www.microsoft.com/en-us/research/project/figureqa-dataset/)

## MODELS

- **VL-T5**  
  Please refer to [VL-T5](https://github.com/vis-nlp/ChartQA/tree/main/Models/VL-T5)

- **T5**  
  Please refer to [T5](https://github.com/vis-nlp/ChartQA/tree/main/Models/T5)

- **VisionTapas**  
  Please refer to [VisionTapas](https://github.com/vis-nlp/ChartQA/tree/main/Models/VisionTapas)





